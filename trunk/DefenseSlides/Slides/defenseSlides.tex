%\documentclass[handout,draft]{beamer}
\documentclass{beamer}
\usetheme{coatneyDis}
\input{include_commands}

\usefonttheme{professionalfonts} % using non standard fonts for beamer
%\usefonttheme{serif} % default family is serif
\usepackage{pgfplots}
\pgfplotsset{compat=1.10}
\usetikzlibrary{cd,shapes,arrows}
\tikzset{>=latex'}

\makeatletter
\newif\ifdraft
\@ifclasswith{beamer}{draft}{\drafttrue}{\draftfalse}
\makeatother

\usepackage{appendixnumberbeamer}
\usepackage{hyperref}

\title[Responsible Softmax]{A Responsibile Softmax Layer in Deep Learning}
\author{Ryan Coatney}
\institute{\tiny{University of Arizona}}

\date{18 June 2018}

\begin{document}
	\maketitle
%	\section{Introduction}
%	\begin{frame}{Abstract}
%		\begin{itemize}
%			\item  \citet{MacKay2002,MML_2019} use \textit{responsibility} in their description of the $K$-means and EM algorithms.
%			\item \alert<2>{\textbf{Dynamic responsibility}} is a related concept that calculates a MLE for class mixture components.
%%			\item Dynamic responsibility is similar to the softmax transfer function used in neural networks.  This connection allows definition of a \alert<3>{\RS} layer for use in classification.
%			\item \alert<3>{\textbf{Responsibility softmax}} enables neural network approximation of underlying distributions.
%			\item Responsibility softmax performs better on imbalanced data than the typical softmax layer.
%		\end{itemize}
%%		We use the concept of responsibility as used in  (and defined in \citet{MacKay2002,MML_2019}) to define a new method of maximum likelihood estimation called \alert<2>{\textbf{dynamic responsibility}}.  is closely related to the softmax function used in neural networks. This relationship may be leveraged to define a new layer called the \alert<3>{\textbf{\RS}} layer.
%%		
%%		\ \\
%%		
%%		The \alert<3>{\textbf{\RS}} layer has an advantage over regular softmax in cases where training and classification is on unbalanced data.  Responsibility softmax layer is also interpretable, but it introduces a new hyperparameter \( C \) that must be tuned for best results.
%	\end{frame}
	
	\section{Clustering and Classification}
	\begin{frame}{Clustering}
		\begin{figure}
			\centering
			\begin{subfigure}{.7\linewidth}
				\input{../kmeansMATLAB/GMMdata}
			\end{subfigure}
			\pause
			\begin{subfigure}{.7\linewidth}
				\ \\
			\end{subfigure}
			\begin{subfigure}{.7\linewidth}
				\input{../kmeansMATLAB/GMMdataclustered}
			\end{subfigure}
		\end{figure}
	\end{frame}

	\begin{frame}{Classification}
		\begin{figure}
			\centering
			\begin{subfigure}{.7\linewidth}
				\input{../kmeansMATLAB/GMMdataclustered}
			\end{subfigure}
			\pause
			\visible<2->{
			\begin{subfigure}{.7\linewidth}
				\ \\
			\end{subfigure}
			\begin{subfigure}{.7\linewidth}
				\input{../kmeansMATLAB/GMMdataclassreg}
			\end{subfigure}
			}
		\end{figure}
	\end{frame}

	\begin{frame}{$K$-means Algorithm}
		Start with data \( \mathcal{X} = \{\bm x^{1},\ldots,\bm x^N\} \) and $K$ starting `means', \( \{\bm m_1,\ldots,\bm m_K\}. \)
		
		\ \\
		
		\alert<2>{\textbf{Assignment:}} For each data point, \(\bm x^{n}\), set 
			\(\hat{k}_n = \argmin_k d(\bm x^{n},\bm m_k)\). Set \alert<3>{\( \rho_i^n = \delta_i^{\hat{k}_n} \) (Hard responsibility)}.
		
		\ \\
		
		\alert<4>{\textbf{Update:}} Let \(N_k = \sum_{n=1}^{N} \rho^n_k\) and 
			\[\bm m_k^{new} = \frac{\sum_{n=1}^{N} \rho_k^n \bm x^{(n)}}{N_k}.\]
		
		If \(d(\bm m_k ,\bm m_k^{new})\) is small for every \( k \), stop.  Otherwise set \(\bm m_k = \bm m_k^{new}\) and \alert<5>{repeat}. See \citet{MacKay2002} for more discussion.
		
	\end{frame}

	\begin{frame}{$K$-means Example}
		\begin{center}
		\only<1>{\begin{figure}\centering\input{../kmeansMATLAB/kmeans_slides/Kmeans5_1}
		\end{figure}}
	\mode<beamer>{
		\only<2>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_2}
		\end{figure}}
		\only<3>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_3}
		\end{figure}}
		\only<4>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_4}
		\end{figure}}
		\only<5>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_5}
		\end{figure}}
		\only<6>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_6}
		\end{figure}}
		\only<7>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_7}
		\end{figure}}
		\only<8>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_8}
		\end{figure}}
		\only<9>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_9}
		\end{figure}}
		\only<10>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_10}
		\end{figure}}
		\only<11>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_11}
		\end{figure}}
		\only<12>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_12}
		\end{figure}}
		\only<13>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_13}
		\end{figure}}
		\only<14>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_14}
		\end{figure}}
		\only<15>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_15}
		\end{figure}}
%		\only<16>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_16}
%		\end{figure}}
%		\only<17>{\begin{figure}\input{../kmeansMATLAB/kmeans_slides/Kmeans5_17}
%		\end{figure}}
	}
		\end{center}
	
	\end{frame}

	\begin{frame}{EM Algorithm on Gaussian Mixture}
		\begin{figure}
%			\centering
			\begin{subfigure}{.9\linewidth}
				\includegraphics[scale=.4]{../kmeansMATLAB/EM3GMM}
				\subcaption{AIC\( \approx 5030 \), BIC\( \approx 5102\)}
			\end{subfigure}
			\pause
			\visible<2->{
				\begin{subfigure}{.9\linewidth}
					\ \\
				\end{subfigure}
				\begin{subfigure}{.9\linewidth}
					\includegraphics[scale=.399]{../kmeansMATLAB/EM5GMM}
					\subcaption{AIC\( \approx 4994 \), BIC\( \approx 5116\)}
				\end{subfigure}
			}
		\end{figure}
	\end{frame}

	\begin{frame}{EM Algorithm on Non-Gaussian Data}
		\only<1>{\begin{figure}
					\centering
					\input{../kmeansMATLAB/CrescentData}
				 \end{figure}
		}
	\mode<beamer>{
		\only<2>{\begin{figure}
					\centering
					\includegraphics[width=0.9\linewidth]{../kmeansMATLAB/EM4Crescent}
				 \end{figure}
		}
		\only<3>{\begin{figure}
					\centering
					\includegraphics[width=0.9\linewidth]{../kmeansMATLAB/EM4CrescentClusterRegions}
				 \end{figure}
		}
		}
	\end{frame}
	
	\section{Dynamic Responsibility}
	\begin{frame}{Responsibility Requirements}
		Necessary items
			\begin{itemize}
				\item Data \( \{\mathcal X,\mathcal T\} = (\bm x^{n},t^{n})\; n=1,\ldots,N \)
				\item Distributions \( f_k(\bm x,\bm\gt_k) \; k=1,\ldots,K\)
				\item Parameter matrix \( F = \left(f_i(\bm x^j,\bm\gt_i)\right)_i^j = (F_i^{j})\)
				\item Mixture probabilities \( \bm\pi_0= \left(\pi_1,\ldots,\pi_K\right)\in S_K \)
			\end{itemize}
		\pause
		\begin{definition}[Probability Simplex]
				\begin{equation*}
				S_K:=\left\{\{\pi_k\}_{k=1}^{K}:0\leq \pi_k\leq 1; \sum_{k=1}^{K}\pi_k =1\right\}.
				\end{equation*}
		\end{definition}
	\end{frame}

	\begin{frame}{Bayes' Rule Estimation of Mixture Probabilities}
		\begin{align*}
		P(t^n=k|\bm x^n, \bm\Theta) &=\dfrac{ P(\bm x^n|t^n=k,\bm\Theta)P(t^n=k|
			 \bm\Theta) }{P(\bm x^n|\bm\Theta)}\\
		&=\dfrac{f_k(\bm x^n,\bm\gt_k)\pi_k} {\sum_{i}\pi_{i}f_{i}(\bm x^n,\bm\gt_i)}
		\end{align*}
	\end{frame}

	\begin{frame}{Responsibility}
		\onslide<1->{Start with rational maps 
			\begin{align*}
			r_i(\bm\pi)=\frac 1N\mathlarger{\sum}_n \frac{\pi_i f_i(\bm x^n,\bm\gt_k)} {\sum_{k}\pi_{k}f_{k}(\bm x^n,\bm\gt_k)}\;\; i=1,\ldots,K
			\end{align*}
		}
		\onslide<2->{
			\begin{definition}<2->[Responsibility Map]
				\begin{equation}\label{map}
				R:S_K\rightarrow S_K: R(\pi_1,\pi_2,\ldots,\pi_K)=(r_1(\bm\pi),r_2(\bm\pi),\ldots,r_K(\bm\pi)).
				\end{equation}
			\end{definition}
			When necessary, write \( R_F(\bm\pi) \) to emphasize dependence on \( K\times N \) parameter matrix \( F \).
		}
	\end{frame}

	\begin{frame}[fragile]{Dynamic Responsibility}
		\begin{algorithm}[H]
			\caption{Dynamic Responsibility Algorithm}\label{ratioAlg}
			\begin{algorithmic}[1]
				\Require $F$ a $K\times N$ matrix
				\Require $\bm\pi_0$, $\ge$ \Comment{$\ge$ creates halt condition}
				\Procedure{Iteration}{$F,\bm\pi_0,\ge$}
				\State $n \gets 1$, $\bm\pi_n \gets R_F(\bm\pi_0)$ 
				\State $orbit \gets \{\bm\pi_0,\bm\pi_1\}$
				\While{$|\bm\pi_n-\bm\pi_{n-1}|>\ge|\bm\pi_n|$}
				\State $\bm\pi_{n+1} \gets R_F(\bm\pi_n)$
				\State $orbit \gets \{\bm\pi_0,\ldots,\bm\pi_{n+1}\}$
				\State $n\gets n+1$
				\EndWhile
				\State \textbf{return} $orbit$ \Comment{at this point $\bm\pi_{n-1}\approx\hat{\bm\pi}$}
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
	\end{frame}

	\begin{frame}{Lyapunov function}
		For $\bm\pi\in\R_+^K$ the positive orthant of \( \R^K \), let 
		\[\ell_F(\bm\pi)={\dfrac{1}{N}}\sum_{n=1}^{N}\log\left(\sum_{k=1}^{K}\pi_kF_k^{n}\right)\]
		\pause
		\begin{lemm}\label{lyapunovLem}
			\( -\elpi{F} \) is a Lyapunov function for dynamic responsibility. In other words,
			\[ \elpi[R_F(\bm\pi)]{F}\geq \elpi{F} \]
			With equality if and only if \( R_F(\bm\pi)=\bm\pi. \)
		\end{lemm}
		\pause
		Note that if \( F \) has full rank, \( -\ell_F \) is \textit{strictly} convex.
	\end{frame}

	\begin{frame}{Main Theorem}
		\begin{theorem}[Convergence of \DR]
			If \( F \) has full rank, and \( \bm\pi_0\in \op{Int}S_K\) then the orbit \( \bm\pi^n = R_F^n(\bm\pi_0) \) converges to \( \hat{\bm\pi}_F, \) the unique maximizing fixed point of \( \elpi{F} \) on \( S_K.\) Moreover, \( \hat{\bm\pi}_F \)  depends differentiably on \( F \).
		\end{theorem}
	\end{frame}

	\begin{frame}{Dynamic Responsibility Example}
	\only<1>{If $F=(F_{i}^j)$ has linearly independent rows, the interior of $S_K$ converges to one point.
		\begin{center}
			\includegraphics[scale=.4]{Full_rank_seed203.png} 
		\end{center}
	}
	\mode<beamer>
	\only<2>{ In this case, convergence happens very quickly. (about 5 iterations)
			\begin{center}
				\includegraphics[scale=.4]{Full_rank_img_seed203.png} 
			\end{center}
		}
	\mode<all>
	\end{frame}
	
	\section{Responsible Softmax}
	\begin{frame}{Calculate \( F \)}
		\Ryan{Needs motivation\\}
		If \( F = e^{\bm A} \) for some \( \bm A =(A_i^j) \) and \( \mu_i = \ln(\pi_i) \), then 
		\[	r_i(\bm\pi)=\frac 1N\mathlarger{\sum}_n \frac{\pi_i F_i^n} {\sum_{k}\pi_{k}F_{k}^n} = \frac 1N \mathlarger{\sum}_n \alert<3>{\frac{\exp({A_i^n+\mu_i})}{\sum_k \exp({A_k^n+\mu_k})}}\]
		\onslide<2->{
		The softmax function is given by the Gibbs Distribution
		\begin{equation*}
		\gs_i(\bm x) = \alert<3>{\frac{\exp({x_i})}{\sum_k \exp({x_k})}}.
		\end{equation*}
		}
		\onslide<4>{This establishes a connection with modern neural networks.}
	\end{frame}	
	
	\begin{frame}{Neural Network Output}
%		\Ryan{Needs motivation}
		Neural networks take in data, and output guesses of cluster assignments.
		\begin{center}
			\(F=(F_i^{j});\;\) \(\;\bm\pi^n=R_F^n(\bm\pi_0);\;\) \(\;\bm\pi^n\rightarrow\hat{\bm\pi} \) as \( n\rightarrow\oo \)
		\end{center}
		\uncover<+->{\[Y(F,\hat{\bm\pi})=\left(\frac{\hat{\pi}_iF_{i}^{j}}{\sum_{k=1}^{K}\hat{\pi}_kF_{k}^{j}}\right)_{i=1,\ldots,K}^{j=1,\ldots,N}\]
		The entry \( Y_i^j \) represents the probability that \( \bm x^j\) comes from cluster \( i \).\\
		}
		\uncover<+->{For some \( F \), it may be that \( \hat{\bm\pi}_F\in\partial S_K \). To prevent this, stop at some finite \(n=C<\oo\) and use \(Y(F,\bm\pi^C)\) as the output.\\ See \citet{NealHintonEM1999} for inspiration.}
	\end{frame}
	
	\begin{frame}{Layer Diagram}
		\begin{figure}
			\centering
				\input{RSgraph}
		\end{figure}
		\only<2>{\begin{center}\( L(\bm Y,\bm{T}) = -\sum_n\sum_k T_k^n\log(Y_k^n) \)
			\end{center}}
	\end{frame}
	
	\begin{frame}{Backpropagation}
		The goal is to use gradient descent to learn parameters of the network.
		\begin{enumerate}
			\item[]\textbf{Option 1:} Automatic differentiation
			\item[]\textbf{Option 2:} Direct calculation
		\end{enumerate}
		\onslide<2>{
			\begin{align}
				D\hat{\bm\pi}_F&=D_{\bm\pi}R\cdot D\hat{\bm\pi}_F+D_FR\nonumber\\
				D\hat{\bm\pi}_F&=\left(I-D_{\bm\pi}R\right)^{-1}\cdot D_{F}R\label{eqn:dPidF}
			\end{align}
			In practice, equation \eqref{eqn:dPidF} is too much.  An approximation may be used instead.
			\only<2>{\( \left(I-D_{\bm\pi}R\right)^{-1}\approx I+DR+DR^2+\ldots+DR^C \)}
		}
	\end{frame}

	\begin{frame}{Setting the Hyperparameter $C$}
%		\only<1>{Recall the dynamic responsibility example. Convergence happened in 5 steps.}
%		\only<2->{
			Let \( a_n = d(\bm\pi_{n+1},\bm\pi_n) \).
			\begin{figure}
				\centering
				\includegraphics[width=0.7\linewidth]{log_dist_nVn}
				\caption{Plot of \(\log(a_n)\) for several \(F\). Each curve represents a different parameter matrix \(F\).}
			\end{figure}
%		}
	\end{frame}

	\section{Basic Experiments}
	\begin{frame}{Experiments with GMM}
		\only<1>{
			\begin{figure}
%				\centering
				\includegraphics[width=0.65\linewidth]{sample2}
				\caption{A sample of data generated from a GMM to test the \RS layer.}
				\label{fig:sample2}
			\end{figure}
		}
	\mode<beamer>
		\only<2>{
				\begin{figure}
					\centering
					\includegraphics[width=0.9\linewidth]{netClassRegions}
					%				\caption{Classification regions for different Neural nets. Net 1 uses the standard softmax layer. Nets 2 and 3 use a \RS layer with \(C=1,4\) respectively. The last layer has fixed weights.}
					\label{fig:netclassregions}
				\end{figure}
%		}
%		\only<3>{	
			
			\begin{table}
			\centering
			\resizebox{.5\linewidth}{!}{
				\begin{tabular}{|l|l|}
					\hline
					\textbf{Net}  & \textbf{Classification layer}\\ \hline
					Net \#1   & Softmax     \\ \hline
					Net \#2   & Responsibility Softmax; \( C=1 \) \\ \hline
					Net \#3   & Responsibility Softmax; \( C=4 \) \\ \hline
					Net \#4   & Fixed Weight Softmax \\ 
					\hline
				\end{tabular}
			}
		\end{table}
		}
		\only<3>{
				\begin{figure}
					\centering
					\includegraphics[width=0.9\linewidth]{deepandWide8_edit}
%					\caption{Various nets trained on similar data. Nets 2 through 5 have $C$ values of $1,4,8,16$ respectively.}
%					\label{fig:deepandwide8edit}
				\end{figure}
				\begin{table}
				\centering
				\resizebox{.45\linewidth}{!}{
					\begin{tabular}{|l|l|}
						\hline
						\textbf{Net}  & \textbf{Classification layer}\\ \hline
						Net \#1   & Softmax     \\ \hline
						Net \#2   & Responsibility Softmax; \( C=1 \) \\ \hline
						Net \#3   & Responsibility Softmax; \( C=4 \) \\ \hline
						Net \#4   & Responsibility Softmax; \( C=8 \) \\ \hline
						Net \#5   & Responsibility Softmax; \( C=16 \) \\ \hline
						Net \#6   & Fixed Weight Softmax \\ 
						\hline
					\end{tabular}
				}
			\end{table}
		}
	\mode<all>
	\end{frame}
	
	\begin{frame}{Non-Gaussian Data Set}
		\only<1>{Recall the performance of the EM algorithm on Crescent data
			\begin{figure}
				\centering
				\includegraphics[width=0.9\linewidth]{../kmeansMATLAB/EM4Crescent}
			\end{figure}
		}
		\mode<beamer>
		\only<2>{
			\begin{figure}
				\centering
				\includegraphics[width=0.9\linewidth]{../kmeansMATLAB/RSCrescentClassification}
				\caption{Classification regions for neural nets trained on crescent data. Hyperparametes are as in GMM example.}
			\end{figure}
		}
		\only<3>{
	
%		}
%		\only<4>{
%	\begin{columns}
%		\begin{column}{.62\textwidth}
			\begin{figure}
%				\centering
				\includegraphics[scale = .28]{../kmeansMATLAB/RSCrescentConfusion}
%				\caption{Confusion matrices for neural nets with \RS layers trained on crescent data.}
			\end{figure}
%		\end{column}
%		
%		\begin{column}{.38\textwidth}
			\begin{table}
%				\centering
				\resizebox{.45\linewidth}{!}{
					\begin{tabular}{|l|l|}
						\hline
						\textbf{Net}  & \textbf{Classification layer}\\ \hline
						Net \#1   & Softmax     \\ \hline
						Net \#2   & Responsibility Softmax \( C=1 \) \\ \hline
						Net \#3   & Responsibility Softmax \( C=4 \) \\ \hline
						Net \#4   & Fixed Weight Softmax \\ 
						\hline
					\end{tabular}
				}
			\end{table}
%		\end{column}
%	\end{columns}
		}
	

\mode<all>
	\end{frame}

	\begin{frame}{Experiments with MNIST}
		\begin{figure}
			\input{benfordConfusion2}
		\end{figure}
	\end{frame}
	

	\begin{frame}{Conclusions}
		We have shown that:
		\begin{itemize}
			\item \textbf{Dynamic responsibility} has nice convergence properties; converges to a MLE.
			\item The \textbf{\RS} layer uses \DR and gives cluster responsibilities.
			\item Using a \RS layer gives better results when working with imbalanced data. It also works when we do not have distributions for the mixture populations.
%			\item The hyperparameter \( C \) should be small in general.
		\end{itemize}
	\end{frame}

	\begin{frame}{Future Work}
		Future work:
		\begin{itemize}
			\item Use \RS with other neural nets, LSTM, VAE, Deductron etc.
			\item Use \RS with nonparametric models (\textit{e.g.} Gaussian processes).
			\item Obtain constructive bounds on convergence rates.
			\item Explore the relationship between hessian of \( \ell_F \) and Fisher Information matrix.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{References}
	    \bibliographystyle{apalike}
		\bibliography{dissertationBib}
%		\printbibliography
	\end{frame}
\appendix
\begin{frame}{EM algorithm for GMM}
	\mode<beamer>
	\only<1>{\begin{align*}
		&\mathcal{X} = \{\bm x^{1},\bm x^{2},\ldots,\bm x^{N}\}  \\
		&f_k(\bm x) \sim \mathcal{N}(\bm \mu_k,\bm \Sigma_k),\;\; k=1,\ldots,K\\
		&p(t^n=k) = \pi_{k}, \;\; \sum_{k} \pi_{k} = 1	
		\end{align*}
	}
\mode<all>
	\only<2>{\begin{enumerate}
			\item \textbf{Expectation} step: Set 
			\begin{equation*}
			\rho_k^n = \frac{\pi_k f_k(\bm x^{(n)})}{\sum_{j=1}^K \pi_j f_j(\bm x^{(n)})}
			\end{equation*}
			\item \textbf{Maximization} step: Set
			\begin{align*}
			&N_k = \sum_{n=1}^{N}\rho_k^{n},&	\bm \mu_{k}^{new} &= \dfrac{1}{N_k}\sum_{n=1}^{N} \rho_k^{n}\bm x^{(n)}\\
			&\pi_k^{new} = \dfrac{N_k}{N},&			
			\bm \Sigma_k^{new} &= \dfrac{1}{N_k}\sum_{n=1}^{N}\rho_k^{n} (\bm x^{(n)}-\bm \mu_{k}^{new})(\bm x^{(n)}-\bm \mu_{k}^{new})^{\intercal}
			\end{align*}
			\item Repeat steps 1 and 2 until convergence.
		\end{enumerate}
		See \citet{BishopBook} for more details.
	}
\end{frame}

\begin{frame}{Proof of Lyapunov Lemma}
\mode<beamer>
	\only<1>{
		\begin{lemm}\label{diffDef}
			The map $R_F(\bm\pi)$ as defined in equation \eqref{map} satisfies
			\[R_F(\bm\pi)=\left(\pi_i\cdot\eval{\frac{\partial\ell_F}{\partial\pi_i}}_{\bm\pi}\right)_{1\leq i\leq K}\]
		\end{lemm}	
	}
\mode<all>
	\only<2-5>{		
		\begin{align*}
		\ell_{F}(R_F(\bm\pi))-\elpi{F} &= \frac{1}{N}\mathlarger{\mathlarger{\sum}_{n=1}^{N}}\log\left\{\frac{\sum_{i=1}^{K}\pi_iF_{i}^{n}\pdv{\ell}{\pi_i}}{\sum_{k=1}^{K}\pi_kF_{k}^{n}}\right\}\\
		&\geq\alert<3>{\mathlarger{\sum}_{n=1}^{N}\sum_{i=1}^{K}\frac 1N \frac{\pi_iF_{i}^{n}} {\sum_{k=1}^{K}\pi_kf_{kn}} \log\left(\pdv{\ell}{\pi_i}\right)}\\
		&=\alert<4>{\mathlarger{\sum}_{i=1}^{K}\sum_{n=1}^{N}}\frac 1N \frac{\pi_iF_{i}^{n}} {\sum_{k=1}^{K}\pi_kf_{kn}} \log\left(\pdv{\ell}{\pi_i}\right)\\
		&= \alert<5>{\sum_{i=1}^{K} r_i(\bm\pi)\log\left(\frac{r_i(\bm\pi)} {\pi_i}\right)\geq 0}
		\end{align*}
	}
\end{frame}

\begin{frame}[fragile]{Confusion for GMM data}
			\only<1>{
				\begin{table}[ht]
					\renewcommand{\arraystretch}{1.4}
					\centering
					%\captionsetup[subtable]{position=top}
				\subcaptionbox{Confusion table for GMM Net \#1.\label{table:GMMconfusion1}}{
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					30.253±.001 & 0.0 & .027±.001   & 0.0 & 0.0         \\ \hline
					1.680±.000  & 0.0 & 0.0         & 0.0 & 0.0         \\ \hline
					.328±.004   & 0.0 & 32.206±.008 & 0.0 & .706±.006   \\ \hline
					0.0         & 0.0 & .033±.001   & 0.0 & 3.207±.001  \\ \hline
					0.0         & 0.0 & .021±.001   & 0.0 & 31.539±.001 \\ \hline
				\end{tabular}
				}
				\caption[Confusion matrices with error estimates for GMM nets \#1-\#4]{The nets were tested on a set of samples drawn independently from the training set. Values are reported as percentages for clarity. Test data sample size \(N=2500\) for all runs. Error intervals are 95\% confidence standard error. An entry of \( 0.0 \) indicates that all values were zero to 3 decimal places.}\label{table:GMMconfusion}
				\end{table}
			}
	\mode<beamer>
			\only<2>{
				\begin{table}[ht]
					\renewcommand{\arraystretch}{1.4}
					\centering
					%\captionsetup[subtable]{position=top}
				\subcaptionbox{Confusion table for GMM Net \#2.\label{table:GMMconfusion2}}{
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					30.165±.004 & .101±.004 & .014±.001   & 0.0       & 0.0         \\ \hline
					1.616±.003  & .063±.003 & 0.0         & 0.0       & 0.0         \\ \hline
					.398±.006   & .114±.003 & 31.739±.010 & .330±.008 & .659±.009   \\ \hline
					0.0         & 0.0       & .031±.001   & .333±.012 & 2.875±.012  \\ \hline
					0.0         & 0.0       & .012±.000   & .082±.004 & 31.466±.004 \\ \hline
				\end{tabular}
				}
				\caption[Confusion matrices with error estimates for GMM nets \#1-\#4]{The nets were tested on a set of samples drawn independently from the training set. Values are reported as percentages for clarity. Test data sample size \(N=2500\) for all runs. Error intervals are 95\% confidence standard error. An entry of \( 0.0 \) indicates that all values were zero to 3 decimal places.}\label{table:GMMconfusion2}
				\end{table}
			}
			\only<3>{
				\begin{table}[ht]
					\renewcommand{\arraystretch}{1.4}
					\centering
					%\captionsetup[subtable]{position=top}
				\subcaptionbox{Confusion table for GMM Net \#3.\label{table:GMMconfusion3}}{
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					29.897±.010 & .374±.010 & .009±.001   & .000±.001  & 0.0         \\ \hline
					1.273±.011  & .406±.011 & .001±.001   & 0.0        & 0.0         \\ \hline
					.658±.016   & .595±.017 & 29.916±.036 & 1.329±.031 & .743±.018   \\ \hline
					0.0         & .000±.001 & .013±.001   & 1.221±.027 & 2.006±.027  \\ \hline
					0.0         & 0.0       & 0.0         & .340±.009  & 31.220±.009 \\ \hline
				\end{tabular}
				}
				\caption[Confusion matrices with error estimates for GMM nets \#1-\#4]{The nets were tested on a set of samples drawn independently from the training set. Values are reported as percentages for clarity. Test 	data sample size \(N=2500\) for all runs. Error intervals are 95\% confidence standard error. An entry of \( 0.0 \) indicates that all values were zero to 3 decimal places.}\label{table:GMMconfusion3}
				\end{table}
			}
			\only<4>{
				\begin{table}[ht]
					\renewcommand{\arraystretch}{1.4}
					\centering
					%\captionsetup[subtable]{position=top}
				\subcaptionbox{Confusion table for GMM Net \#4.\label{table:GMMconfusion4}}{
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					26.842±.035 & 3.438±.035 & 0.0         & 0.0        & 0.0         \\ \hline
					.044±.006   & 1.636±.006 & 0.0         & 0.0        & 0.0         \\ \hline
					.075±.003   & 1.841±.024 & 28.737±.037 & 2.540±.025 & .047±.002   \\ \hline
					0.0         & 0.0        & .027±.001   & 3.122±.004 & .092±.004   \\ \hline
					0.0         & 0.0        & 0.0         & 2.463±.023 & 29.097±.023 \\ \hline
				\end{tabular}
			}
			\caption[Confusion matrices with error estimates for GMM nets \#1-\#4]{The nets were tested on a set of samples drawn independently from the training set. Values are reported as percentages for clarity. Test data sample size \(N=2500\) for all runs. Error intervals are 95\% confidence standard error. An entry of \( 0.0 \) indicates that all values were zero to 3 decimal places.}\label{table:GMMconfusion4}
			\end{table}
		}
	\mode<all>
	\end{frame}

	\begin{frame}{Per class precision and recall for GMM data}
		\only<1>{
		\begin{table}[ht]
			\renewcommand{\arraystretch}{1.4}
			\centering
			\subcaptionbox{Precision and Recall table for GMM Net \#1.\label{table:GMMprecRec1}}[.45\linewidth]{
				\begin{tabular}{|c|c|c|}
					\hline
					\multicolumn{3}{|c|}{\textbf{GMM Net 1}} \\ \hline
					Class      & Precision      & Recall     \\ \hline
					1          & 0.936          & 0.999      \\ \hline
					2          & 0.000          & 0.000      \\ \hline
					3          & 0.998          & 0.966      \\ \hline
					4          & 0.000          & 0.000      \\ \hline
					5          & 0.979          & 0.999      \\ \hline
				\end{tabular}
			}
			\caption[Per class precision and recall for GMM nets \#1-\#4]{This table shows per class precision and recall for GMM nets trained and tested on the same data as in table \ref{table:GMMconfusion}}\label{table:GMMprecRec}
		\end{table}
	}
	\mode<beamer>
	\only<2>{
		\begin{table}[ht]
			\renewcommand{\arraystretch}{1.4}
			\centering
			\subcaptionbox{Precision and Recall table for GMM Net \#2.\label{table:GMMprecRec2}}[.45\linewidth]{
				\begin{tabular}{|c|c|c|}
					\hline
					\multicolumn{3}{|c|}{\textbf{GMM Net 2}} \\ \hline
					Class      & Precision      & Recall     \\ \hline
					1          & 0.935          & 0.997      \\ \hline
					2          & 0.209          & 0.027      \\ \hline
					3          & 0.998          & 0.953      \\ \hline
					4          & 0.401          & 0.090      \\ \hline
					5          & 0.898          & 0.997      \\ \hline
				\end{tabular}
			}
			\caption[Per class precision and recall for GMM nets \#1-\#4]{This table shows per class precision and recall for GMM nets trained and tested on the same data as in table \ref{table:GMMconfusion}}\label{table:GMMprecRec2}
		\end{table}
	}
	\only<3>{
		\begin{table}[ht]
			\renewcommand{\arraystretch}{1.4}
			\centering
			\subcaptionbox{Precision and Recall table for GMM Net \#3.\label{table:GMMprecRec3}}[.45\linewidth]{
				\begin{tabular}{|c|c|c|}
					\hline
					\multicolumn{3}{|c|}{\textbf{GMM Net 3}} \\ \hline
					Class      & Precision      & Recall     \\ \hline
					1          & 0.934          & 0.988      \\ \hline
					2          & 0.279          & 0.194      \\ \hline
					3          & 0.999          & 0.894      \\ \hline
					4          & 0.385          & 0.364      \\ \hline
					5          & 0.918          & 0.989      \\ \hline
				\end{tabular}
			}
			\caption[Per class precision and recall for GMM nets \#1-\#4]{This table shows per class precision and recall for GMM nets trained and tested on the same data as in table \ref{table:GMMconfusion}}\label{table:GMMprecRec3}
		\end{table}
	}
	\only<4>{
		\begin{table}[ht]
			\renewcommand{\arraystretch}{1.4}
			\centering
			\subcaptionbox{Precision and Recall table for GMM Net \#4.\label{table:GMMprecRec4}}[.45\linewidth]{
				\begin{tabular}{|c|c|c|}
					\hline
					\multicolumn{3}{|c|}{\textbf{GMM Net 4}} \\ \hline
					Class      & Precision      & Recall     \\ \hline
					1          & 0.988          & 0.930      \\ \hline
					2          & 0.289          & 0.882      \\ \hline
					3          & 0.999          & 0.868      \\ \hline
					4          & 0.380          & 0.969      \\ \hline
					5          & 0.996          & 0.922      \\ \hline
				\end{tabular}
			}
			\caption[Per class precision and recall for GMM nets \#1-\#4]{This table shows per class precision and recall for GMM nets trained and tested on the same data as in table \ref{table:GMMconfusion}}\label{table:GMMprecRec4}
		\end{table}
	}
\mode<all>
\end{frame}

\end{document}