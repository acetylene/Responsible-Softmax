%!TEX root = DissertationDraft.tex
\label{supVunsup}
Within the realm of machine learning there are two broad collections of 
algorithms known as supervised and unsupervised learning. While each set of 
algorithms has their own uses and drawbacks, they are often compared as if 
they were the two extremes of a spectrum. The practice of employing algorithms 
in these two categories is often more nuanced.

Supervised machine learning requires large amounts of labeled data 
$\bm{D}=\{\bm{X},\bm{T}\}$.  Here the data points \( \bm X :=\{\bm x^{n}\}\) are drawn from some underlying probability space \( \mathcal{X} \). The data also has an extra feature 
$\bm{T}:=\{\bm t^n\}$, drawn from a set \( \mathcal{T} \), that acts as labels for individual data points. Many practical situations suppress direct discussion of the sets \( \mathcal{X},\mathcal{T} \) in favor of talking about what the data implies about the underlying sets.

The labels \(\{\bm t^n\}\) may be categorical, as when we are trying to classify data points.  
$\bm{T}$ may also be output of some unknown function on which we wish to 
perform regression. For the remainder of this paper, we will consider the 
classification problem but regression remains a good source of inspiration.

In either case, the goal of supervised learning is to develop a program that
correctly predicts the label $t'$ of a given data point $x'$ which the algorithm has not seen before. In the case of classification problems, the algorithm gives a set of 
probabilities $P(t'=\ell|x')$ as $\ell$ ranges over the finite set of 
classification categories which we will call $\mathcal{C}$. A reasonable 
constraint in this situation is to require that 
\[\sum_{\ell\in\mathcal{C}}P(t'=\ell|x') = 1.\]

In light of the above discussion it is effective when considering supervised 
learning to view the problem as an estimation of the conditional probability 
$P(\bm{X}|\bm{T})$. We may then use Bayes' Rule to find 
\[P(\bm T|\bm X)\propto P(\bm X|\bm T)\cdot P(\bm X).\]


As part of this process, it is typical to choose a loss (or cost) function 
$L:\mathcal{X}\times\mathcal{T}\rightarrow \R$.  The probability 
$P(\bm{X}|\bm{T})$ is then estimated by minimization of the chosen 
loss function. Use of the known labels \( \bm T \) and a given loss function are key features of supervised learning. Common supervised learning algorithms are support vector machines, naive Bayes, logistic regression, and  neural networks such as the multilayer perceptron.

The basic ideas behind supervised learning can be more fully explored through 
the example of the multilayer perceptron.  This discussion follows the explanation given in 
Bishop \cite{BishopBook}. This model is discussed in chapter 5 of Bishop, 
and there it is also called the feed-forward neural network. It is closely 
related to, and simpler than, the `deep' learning in common use today.

For this specific example, suppose that \(\mathcal{X}=\{\bm x^{(n)}\}\), 
with \(\bm x^{(n)}\in \R^d\) for \(n=1\ldots N\). Recall that the goal of 
supervised classification is to make an appropriate approximation of the 
distribution \(P(\mathcal{T}|\mathcal{X})\). 

The way a multilayer perceptron does this is through composing two or more
layers to perform inference.  Each layer can be viewed as a many logistic 
regression algorithms working together to pass appropriate information on to 
the next layer.  The final layer is called the loss layer, and it has the 
responsibility of measuring how far the outputs of the neural network are
from the ideal distribution.

Unsupervised learning, on the other hand, seeks to find patterns in the data
without the requirement of labels.  One set of unsupervised learning 
algorithms are clustering algorithms.  These algorithms seek to find patterns 
among the data and group the data points according to these patterns. 

Among clustering algorithms, we wish to pay most attention to mixture modeling.
While mixture modeling is useful for more than just clustering, it is 
worthwhile to think of them as a clustering algorithms to begin with.  Two 
algorithms for mixture models on which we will focus are the $K$-means algorithm and the 
Expectation Maximization (EM) algorithm.  While we will focus on each of these 
algorithms in detail in sections \ref{kmeans} and \ref{emAlg}, at this point we will 
discuss some of the common details.

First, as in the introduction, all mixture models suppose that the data is sampled from $K<\oo$ 
different distributions modeled by the distributions 
$f_k(\bm x,\bm \theta_k)$, $k=1\ldots K$. Here the $\bm\theta_k$ are 
distribution specific parameters. We then form a model $p(\bm x)$ by taking a 
convex combination of the given distributions,
\begin{equation}\label{mixProb}
		P(\bm x;\bm\pi,\bm\Theta)=\sum_{k=1}^{K}\pi_kf_k(\bm x,\bm\theta_k).
\end{equation}

Where we require that $\sum_k \pi_k =1$ and 
$\bm\Theta = \{\bm\gt_1\ldots\bm\gt_K\}$. The goal then of mixture models is 
to determine $\{\bm\pi,\bm\Theta\}$ from the given data. In this case, equation \ref{mixProb} is the same as equation \ref{eqn:mixtureDist}.  One important example of a mixture model is the Gaussian Mixture Model (GMM), which assumes that the distributions \( f_k(\bm x,\bm \mu_k,\bm \Sigma_k) \) ar all normal distributions with possibly unknown means and covariance matrices.

We recall from section \ref{classvCluster} that clustering and classification are two closely 
related but different problems. Clustering seeks to infer a distribution for 
the various clusters in the data. Classification looks to label the data 
points according to membership in various clusters. Both the $K$-means and EM 
algorithms have a semi-classification step which we will refer to as 
responsibility assignment. \cite{BishopBook,MML_2019}

In the EM algorithm, these responsibility assignments are often referred to as 
latent variables. The mixing constants $\bm\pi$, may also be considered latent 
variables, but as will be seen, responsibility is closely related to the 
mixing constants.

%how is responsibility used? $N=|D|$, $N_k=\#\{x\in D|\text{ class of } x=k\} 
%= \sum_n r_k^{(n)}$, $\lim_{N\rightarrow\oo}\frac{N_k}{N} = \pi_k$
We first give a definition of responsibility.  In its simplest form, 
responsibility is the cluster assignment for a point in one iteration of the 
$K$-means or EM algorithm. If $N = |\mathcal{D}|$ is the number of data 
points, and  $K$ is the number of clusters,then 
$r^{(n)}_k,\ 1\leq n\leq N,\ 1\leq k\leq K$ is the responsibility of the 
$K$-th cluster for the data point $\bm x^{(n)}$.  

In the most basic implementation, $r^{(n)}_k \in \{0,1\}$. Explicitly, we have 
$r^{(n)}_k=1$ if $x^{(n)}$ is assigned to cluster $k$ and $r^{(n)}_k = 0$ 
otherwise. We will call this \textit{hard responsibility}. As a slight 
modification, we may also consider the case where $r^{(n)}_k \in [0,1]$. In 
this case we require that $\sum_k r^{(n)}_k = 1$. We will call this 
\textit{soft responsibility}.
 
The total responsibility for the cluster $k$ is the value 
$N_k = \sum_n r^{(n)}_k$.
Whether we are working with hard or soft responsibility, the relative 
responsibility of cluster $k$ is $\frac{N_k}{N}$.  The mixing probability is 
approximated by the relative responsibility as will be discussed further in 
section \ref{respMLE}.

%Input monte carlo discussion here? If I can generate samples, then 
As a brief discussion of the connection between responsibility and mixing 
constants, recall experiment \ref{exper:MCMixSample}.   
% the following informal two stage experiment.  We select a 
%point in $\bm x^{(n)}\in\R^d$ via the following process:
%\begin{enumerate}
%\item Stage 1: From the $K$ possible distributions, select a label $k_n$ with 
%probability $P(k_n=k)=\pi_{k}$.
%\item Stage 2: Sample $x_n$ from $f_{k_n}(x)$
%\end{enumerate}
As the number of samples generated by this Monte Carlo sampling grows, the following holds. \[\lim_{N\rightarrow\oo}\frac{N_k}{N} = \pi_k.\] 
%In the Monte Carlo simulation we have set up above, this is the case.

%In brief summary, 
