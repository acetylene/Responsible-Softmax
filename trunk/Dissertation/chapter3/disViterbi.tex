The Viterbi algorithm was proposed by Andrew Viterbi in 1967, though many other algorithms that are similar in nature were proposed at the same time \cite{ForneyVPH,ViterbiPH}.  In general, the Viterbi algorithm is an instance of a message passing algorithm, which uses dynamic programming and an inherent graph structure to determine the maximum likelihood decoding of an incoming signal transmission over a noisy channel.  What this means in our context is that different symbols were being transmitted in a manner that introduced error, and the Viterbi algorithm classifies each received symbol in a manner to increase the likelihood of correct classification.  This technique is used in channel coding, and a full understanding requires some discussion of error correcting codes.  However, the Viterbi algorithm has seen use in many varied fields \cite{ViterbiPH}, and it is in the context of classification that we will discuss it.

\begin{figure}[h]
\begin{tikzpicture}

\node[state] (1) {A};
\node[state, right=of 1] (2) {B};

\draw[every loop]
            (1) edge[bend right, auto=left]  node [below] {0.6} (2)
			(2) edge[bend right, auto=right] node {0.7} (1)
			(1) edge[loop left]             node {0.4} (1)
			(2) edge[loop right]             node {0.3} (2);
\end{tikzpicture}
\caption{A simple Example of a Graph for a Markov chain}\label{markovGraph}
\end{figure}
\subsection{Markov Chains}
To begin we will do a brief review of finite state Markov processes, also often called Markov chains. Let $G_M$ be a weighted graph representing a Markov chain as in figure \ref{markovGraph}. In this case, the vertices of $G_M$ represent possible states of the Markov chain, the edge weights represent transition probabilities.  
We recall that the definition of a Markov process is that the probability of the next event depends only on the state of the previous event.  That is, in mathematical terms $p(X_n|X_{n-1},\ldots,X_0)=p(X_n|X_{n-1})$. The connection to the graph \ref{markovGraph} is concept of taking a ``random walk" on the graph $G_M$.  Starting from some initial state e.g. $X_0=A$, we move to the next state along each outgoing edge $e_i$ with probability $w_i$, where $w_i$ is the weight of edge $e_i$.  In the example above, if we set $X_0=A$, $P(X_1=A|X_0)=.4$ and $p(X_1=B|X_0)=.6$.
 
Now if we number the possible states (or graph vertices) $1, 2,\ldots, d$ we can create the weighted adjacency matrix $A_M$ by setting $a_{ij}=w_{ij}$, where $w_{ij}$ is the weight of the edge going \textit{from} vertex $i$ \textit{to} vertex $j$.  If there is no such edge, we set $w_{ij}=0$.  By construction, each column of $A_M$ sums to 1, i.e. $\sum_i a_{ij}=1\; \forall j$.  We call such a matrix a stochastic matrix.

We call a weighted directed graph $G$ \textit{strongly connected} if there is a directed path in $G$ from each vertex of $G$ to the every other vertex in $G$.  In the case where such a graph $G_m$ is associated with a Markov chain, we call that Markov chain  \textit{irreducible}.  If $A_M$ is the associated adjacency matrix as above, these definition are equivalent to saying that for each pair, $i,j$ there is some integer $n$ such that $\left(A_M^n\right)_{ij}\neq 0$.

The Perron-Frobenius theorem gives us the following well known result: The stochastic, irreducible matrix $A_M$ has a unique probability vector $\bm\pi$ with $\sum_{i}\pi_i=1$, such that $\bm\pi$ is an eigenvector of $A_M$ with eigenvalue 1. All other eigenvalues of $A_M$ have modulus less than 1.  We call this probability vector $\bm\pi$ the \textit{steady state vector} of the Markov chain associated with $A_M$

The bulk of this material can be found in a good book on Markov processes, such as \textcolor{red}{(INSERT REFERENCE)}.\\
%What does this have to do with viterbi decoding?

\subsection{Viterbi Algorithm}
Suppose now, that we have some data generated by a Markov chain, but we have some error in how the data is received.  The Viterbi algorithm will help us determine the maximum likelihood for the state of each data point.  In addition, if we are getting the data as it is being generated, the Viterbi algorithm works well in an `online' manner.