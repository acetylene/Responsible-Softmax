This is a placeholder for a discussion about the softmax and Log Sum Exp functions.  They will be closely related to the process of determining the matrix $F$ through learning (gradient descent).

note: On $\R^2$ softmax looks like logistic regression, can be generalized.  Also there is a connection between LSE and softmax, described in bishop:NN.
(what is log of softmax?) 

late night notes:
does relationship look like:
\[f(\argmax)=\max f(x)?\]
maybe not in general, as one on the left may be a set, and the one on the right isn't guaranteed to exist.
However, it might be nice to see a similar relationship with softmax and LSE.