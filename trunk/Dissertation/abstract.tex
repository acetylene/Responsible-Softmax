%!TEX root = ../Dissertation_RC.tex

%\section{Abstract}

Clustering algorithms are an important part of modern data analysis.  The K-means and EM clustering algorithm both use an iterative process to find latent (or hidden) variables in a mixture distribution.  These Hidden variables may be interpreted as class label for the data points of a sample.  In connection with these algorithms, we consider a family of nonlinear mappings called \textit{responsibility} maps. The responsibility map is obtained as a gradient of the log likelihood of $N$ independent samples, drawn from a mixture of $K$ distributions.  We look at the discrete dynamics of this family of maps and give a proof that iteration of responsibility converges to an estimate of the mixing coefficients.  We also show that the convergence is consistent in the sense that the fixed point acts as a maximizer of the log likelihood.

We call the process of determining class weight by iteration \textit{dynamic responsibility} and show that it converges to a unique set of weights under mild assumptions. Dynamic responsibility (DR) is inspired by the expectation step of the expectation maximization (EM) algorithm and has a useful association with Bayesian methods.  Like EM, dynamic responsibility is an iterative algorithm, but DR will converge to a unique maximum under reasonable conditions.  The weights determined by DR can also be found using gradient descent but DR guarantees non-negative weights and gradient descent does not.

We present a new algorithm which we call \textit{responsible softmax} for doing classification with Neural networks.  This algorithm is intended to handle imbalanced training sets and is accomplished via multiplication by per class weights.  These weights may be interpreted as class probabilities for a generalized mixture model, and are determined through DR rather than by empirical observation of the training set and heuristically selecting the underlying probability distributions.

We compare the performance of responsible softmax with other standard techniques, including standard softmax, and weighted softmax using empirical class probabilities.  We use generated Gaussian mixture model data and the MNIST data set for proof of concept.  We show that in general, responsible softmax produces more useful classifiers than softmax when presented with imbalanced training data.  It will also be seen that responsible softmax approximates the performance of empirically weighted softmax, and in some cases may do better.