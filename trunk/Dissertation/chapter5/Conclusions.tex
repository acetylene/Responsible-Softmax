\label{sect:conclusions}
This dissertation defined \DR and showed that the only stable point of \DR the is MLE of the log likelihood function for a discrete mixture model. Future studies could provide further exploration of convergence rate, and bounds on singular values for families of the matrix of parameters \( F \). Another option for future exploration is to use Newton-Kantorovic method on \( R_F \) to get better convergence bounds.  Newton methods can give negative weights, but exploration of how to prevent this could provide better comparison to dynamic responsibility.  This line of thought reflects the original question that lead to this research.

Having proved good properties of dynamic responsibility, this paper then defined \RS, computed derivatives for backpropagation, showed that  the fixed point \( \hat{\bm \pi}_F \) varies smoothly with \( F \), and found a connection  between empirical convergence rate and the singular values of \( F \). Numerical tests were performed on some simple data sets with good results. Future studies could identify and try other data sets. The adaptability of \RS would also allow experiments with other neural net architectures, \textit{e.g.} LSTM, VAE, MOE. Given the general assumptions on \( F \), \RS might also work well with nonparametric methods, like Gaussian Processes.

Much of the analysis of \DR and the \RS layer required understanding of the Fr\'{e}chet derivative \( DR_F \). The close relation that \( DR_F \) has to the second derivative of the log likelihood function \( \ell_{F} \) implies a similarly close relation to the Fisher information matrix \( \mathcal{I}(\bm\pi) = -E\left[\left.\pdv[2]{\ell_F}{\bm\pi}\right|\bm\pi\right] \). Exploration of this connection beyond what is mentioned in section \ref{respMLE} could be interesting on its own.  This would require more assumptions on the distributions \( f_k(\bm x,\bm\gt_k) \), but may provide additional further avenues of research.

%Friday 12 Feb 2016 - Tuesday, 1 June 2020